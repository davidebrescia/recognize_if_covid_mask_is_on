{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzsy5DqK_qHc"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
        "\n",
        "# method used for splitting the dataset into training and validation set\n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NIPEtKVTdDF"
      },
      "source": [
        "##Mounting the Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hps6zVFO_tUR",
        "outputId": "a7f4fe5f-5c0e-4bed-ca74-96fd6d720ec3"
      },
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "dataset_dir = os.path.join(cwd, '/content/drive/My Drive/no_mask_detector/dataset')\n",
        "\n",
        "decide_class_indices = True\n",
        "if decide_class_indices:\n",
        "  classes = [ '0',         # no_person\n",
        "              '1',         # all_the_people\n",
        "              '2' ]        # someone\n",
        "else:\n",
        "  classes = None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wr60TIqTiwZ"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVW5bSEX_vbY"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "apply_data_augmentation = True\n",
        "\n",
        "# Create training ImageDataGenerator object with data augmentation\n",
        "if apply_data_augmentation:\n",
        "  train_data_gen = ImageDataGenerator(rotation_range=20,\n",
        "                                      width_shift_range=20,\n",
        "                                      height_shift_range=20,\n",
        "                                      zoom_range=0.2,\n",
        "                                      horizontal_flip=True,\n",
        "                                      vertical_flip=True,\n",
        "                                      fill_mode='constant',\n",
        "                                      cval=0,\n",
        "                                      preprocessing_function=preprocess_input)\n",
        "else:\n",
        "  train_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Create validation ImageDataGenerator object\n",
        "valid_data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRu_JGnmTmr5"
      },
      "source": [
        "##Training and Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thPWnRU5GSj3"
      },
      "source": [
        "# Folders for the train and test set\n",
        "\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "test_dir = os.path.join(dataset_dir, 'test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RP-nnEdfDRiH"
      },
      "source": [
        "# Dividing the dataset into training and validation set\n",
        "\n",
        "bs = 16\n",
        "num_classes = 3\n",
        "\n",
        "# We decide to set the image width and height to 612x408 cause we saw it was the most common image size on both the train and test set\n",
        "img_w = 612\n",
        "img_h = 408\n",
        "\n",
        "# Set the random seed\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "# Loading json file into a pandas dataframe\n",
        "with open(os.path.join(dataset_dir,\"train_gt.json\")) as f:\n",
        "  dic = json.load(f)\n",
        "\n",
        "dataframe = pd.DataFrame(dic.items())\n",
        "dataframe.rename(columns = {0:'filename', 1:'class'}, inplace = True)\n",
        "dataframe[\"class\"] = dataframe[\"class\"].astype(str)\n",
        "\n",
        "# Create the training and validation dataframe\n",
        "train_dataframe = pd.DataFrame()\n",
        "valid_dataframe = pd.DataFrame()\n",
        "\n",
        "# Splitting the dataset into training and validation with respectively 90% and 10% of the entire samples\n",
        "train_dataframe, valid_dataframe = train_test_split(dataframe, test_size = 0.1, shuffle= True)\n",
        "\n",
        "\n",
        "# Training\n",
        "train_gen = train_data_gen.flow_from_dataframe(train_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               target_size=(img_w,img_h),\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "\n",
        "# Validation\n",
        "valid_gen = valid_data_gen.flow_from_dataframe(valid_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               target_size=(img_w,img_h),\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfsBGzSRGWjz"
      },
      "source": [
        "# Creating the training set and validation set from the generators\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_w, img_h, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_w, img_h, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PtTtNGwTu2N"
      },
      "source": [
        "##Model Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8JknFoXGgoD"
      },
      "source": [
        "# Loading the ResNet152V2 model\n",
        "\n",
        "# We used the GAP to limit the overfitting by reducing the number of parameters of our model\n",
        "# We didn't include the top layers to make us build our own fully connected layers\n",
        "resnet = tf.keras.applications.ResNet152V2(include_top=False,\n",
        "                                          weights=\"imagenet\",\n",
        "                                          input_shape=(img_w, img_h, 3),\n",
        "                                          pooling = 'avg')\n",
        "\n",
        "resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZW5bb-bI4UL"
      },
      "source": [
        "# Create Model\n",
        "\n",
        "# Fine tuning\n",
        "\n",
        "finetuning = True\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 15 # layer from which we want to fine-tune\n",
        "    \n",
        "    for layer in resnet.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    resnet.trainable = False\n",
        "    \n",
        "model = tf.keras.Sequential()\n",
        "model.add(resnet)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(.4))\n",
        "\n",
        "# Kernel_regularizer to have another method for limiting the overfitting\n",
        "model.add(tf.keras.layers.Dense(units=256, activation='relu', kernel_regularizer='l2'))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# Visualize created model as a table\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNljzDaGJFO2"
      },
      "source": [
        "# Optimization parameters\n",
        "\n",
        "\n",
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "\n",
        "# Validation metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTiI5YuUJTIZ"
      },
      "source": [
        "# Callbacks\n",
        "callbacks = []\n",
        "\n",
        "\n",
        "#Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    \n",
        "    # We decide to put the restore_best_weights of the EarlyStopping callback to true in order to have the best performing model \n",
        "    # after the training phase and not the last one before stopping\n",
        "    \n",
        "    # We put the patience to 8 to wait a little bit more for hypothetical small decreasing on the loss of the validation set\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrdzWw4oUAmS"
      },
      "source": [
        "##Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sppP30qHNrt2"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "# The 2 quantities below are the step size per epoch for respectively the training set and validation set\n",
        "# We chose those quantities to make the steps depending on the number of samples on the set and on the batch size\n",
        "STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
        "STEP_SIZE_VALID=valid_gen.n//valid_gen.batch_size\n",
        "\n",
        "model.fit(x=train_dataset,\n",
        "          epochs=200,\n",
        "          steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=STEP_SIZE_VALID,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN7fFu0GUCvt"
      },
      "source": [
        "##Model Prediction and save results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-av9du9OOnv"
      },
      "source": [
        "# Model Prediction\n",
        "image_filenames_test = next(os.walk(test_dir))[2]\n",
        "\n",
        "# dictionary for the predictions made at each iteration of the following for cycle\n",
        "predictions = {}\n",
        "for image_filename in image_filenames_test:\n",
        "\n",
        "  # Image loading and converting to RGB mode\n",
        "  img = Image.open(os.path.join(test_dir,image_filename)).convert('RGB')\n",
        "\n",
        "  # Resizing it to make it suitable to the model structure\n",
        "  img = img.resize((img_h,img_w))\n",
        "  img_array = np.array(img)\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "\n",
        "  # Preprocessing to make it suitable to the resnetV2 architecture\n",
        "  img_array = preprocess_input(img_array)\n",
        "\n",
        "  prediction = model.predict(img_array)\n",
        "\n",
        "  # Taking the argmax of the predictions made to have the class predicted\n",
        "  predictions[image_filename] = np.argmax(np.matrix(prediction))\n",
        "\n",
        "\n",
        "# Function to create the CSV file with the results\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')\n",
        "\n",
        "\n",
        "# Create the CSV file from the predictions made and save it on our folder on Drive\n",
        "create_csv(predictions, '/content/drive/My Drive/no_mask_detector/results')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}