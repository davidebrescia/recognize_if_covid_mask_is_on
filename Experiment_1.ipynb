{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Experiment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UerX1a1VhvgF"
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBe-FZJ0qRXb"
      },
      "source": [
        "##Mounting the Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nriET7yCbn"
      },
      "source": [
        "drive.mount('/content/drive/')\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "dataset_dir = os.path.join(cwd, '/content/drive/My Drive/no_mask_detector/dataset')\n",
        "\n",
        "decide_class_indices = True\n",
        "if decide_class_indices:\n",
        "  classes = [ '0',         # no_person\n",
        "              '1',         # all_the_people\n",
        "              '2' ]        # someone\n",
        "else:\n",
        "  classes = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMaPWkBYrB2d"
      },
      "source": [
        "##Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXke6Pb0x3xb"
      },
      "source": [
        "# Data augmentation\n",
        "apply_data_augmentation = True\n",
        "\n",
        "if apply_data_augmentation:\n",
        "  train_data_gen = ImageDataGenerator(width_shift_range=10,\n",
        "                                      height_shift_range=10,\n",
        "                                      zoom_range=0.1,         # Not too wide zoom range, in order not to lose the correct 'label'\n",
        "                                      horizontal_flip=True,   # Vertical flip is useless, we don't expect upside down images\n",
        "                                      fill_mode='nearest',\n",
        "                                      rescale=1./255)\n",
        "else:\n",
        "  train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfybOioQrNYo"
      },
      "source": [
        "##Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yejt-9L3yGPK"
      },
      "source": [
        "# Data Loading\n",
        "bs = 8\n",
        "num_classes = 3\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "\n",
        "# Loading json file into a pandas dataframe\n",
        "with open(os.path.join(dataset_dir,\"train_gt.json\")) as f:\n",
        "  dic = json.load(f)\n",
        "\n",
        "dataframe = pd.DataFrame(dic.items())\n",
        "dataframe.rename(columns = {0:'filename', 1:'class'}, inplace = True)\n",
        "dataframe[\"class\"] = dataframe[\"class\"].astype(str)\n",
        "dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Dividing dataframes into \"train_dataframe\" and \"valid_dataframe\" by equally\n",
        "# distributing saples of the main dataframe among the different classes.\n",
        "# Proportions: \n",
        "# train_dataframe --> first 80% of the samples.\n",
        "# valid_dataframe --> last 20% of the samples.\n",
        "train_dataframe = pd.DataFrame()\n",
        "valid_dataframe = pd.DataFrame()\n",
        "\n",
        "for i in range(num_classes):\n",
        "  df = dataframe.loc[dataframe['class'] == str(i)]\n",
        "  train_dataframe = train_dataframe.append(df.head(math.floor(len(df.index) * 0.8)), ignore_index=True)\n",
        "  valid_dataframe = valid_dataframe.append(df.tail(math.floor(len(df.index) * 0.2)), ignore_index=True)\n",
        "\n",
        "\n",
        "# Reshuffle of the dataframes\n",
        "train_dataframe = train_dataframe.sample(frac=1).reset_index(drop=True)\n",
        "valid_dataframe = valid_dataframe.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Training\n",
        "training_dir = os.path.join(dataset_dir, 'training')\n",
        "train_gen = train_data_gen.flow_from_dataframe(train_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               target_size=(img_h,img_w),\n",
        "                                               shuffle=True,\n",
        "                                               seed=SEED)\n",
        "\n",
        "# Validation\n",
        "valid_gen = valid_data_gen.flow_from_dataframe(valid_dataframe,\n",
        "                                               training_dir,\n",
        "                                               batch_size=bs,\n",
        "                                               classes=classes,\n",
        "                                               class_mode='categorical',\n",
        "                                               target_size=(img_h,img_w),\n",
        "                                               shuffle=False,\n",
        "                                               seed=SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSZSL8aorWC0"
      },
      "source": [
        "##Training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_NgZ6jQzQvX"
      },
      "source": [
        "# Datasets\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAqmHZYYrb89"
      },
      "source": [
        "##Model creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xlU9O1S0jNA"
      },
      "source": [
        "# Model\n",
        "start_f = 8\n",
        "depth = 5\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "# Features extraction\n",
        "for i in range(depth):\n",
        "\n",
        "    if i == 0:\n",
        "        input_shape = [img_h, img_w, 3]\n",
        "    else:\n",
        "        input_shape=[None]\n",
        "\n",
        "    # Conv block: Conv2D -> Activation -> Pooling\n",
        "    model.add(tf.keras.layers.Conv2D(filters=start_f, \n",
        "                                    kernel_size=(3, 3),\n",
        "                                    strides=(1, 1),\n",
        "                                    padding='same',\n",
        "                                    input_shape=input_shape))\n",
        "    model.add(tf.keras.layers.ReLU())\n",
        "    model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2)))\n",
        "\n",
        "    start_f *= 2\n",
        "    \n",
        "# Classifier\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(.3))\n",
        "model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "# Visualize initialized weights\n",
        "model.weights\n",
        "\n",
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "\n",
        "# Metrics\n",
        "metrics = ['accuracy']\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RezPvh3arkPH"
      },
      "source": [
        "##Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MumCCNQ03Z9Z"
      },
      "source": [
        "# Tensorboard initialization\n",
        "exps_dir = os.path.join('/content/drive/My Drive/no_mask_detector', 'classification_experiments')\n",
        "if not os.path.exists(exps_dir):\n",
        "    os.makedirs(exps_dir)\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/drive/My\\ Drive/Kaggle\\ Competitions/Competition1/classification_experiments/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCyOFqKrs5I"
      },
      "source": [
        "##Checkpoint and callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er-KaU_P1nuW"
      },
      "source": [
        "# Checkpoint and callbacks\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'CNN'\n",
        "\n",
        "exp_dir = os.path.join(exps_dir, model_name + '_' + str(now))\n",
        "if not os.path.exists(exp_dir):\n",
        "    os.makedirs(exp_dir)\n",
        "\n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = os.path.join(exp_dir, 'ckpts')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "tb_dir = os.path.join(exp_dir, 'tb_logs')\n",
        "if not os.path.exists(tb_dir):\n",
        "    os.makedirs(tb_dir)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=1)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    # We decide to put the restore_best_weights of the EarlyStopping callback to true in order to have the best performing model \n",
        "    # after the training phase and not the last one before stopping\n",
        "    \n",
        "    # We put the patience to 7 to wait a little bit more for hypothetical small decreasing on the loss of the validation set\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n",
        "    callbacks.append(es_callback)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsfJx2YoryeU"
      },
      "source": [
        "##Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUAwHSY-1Z3s"
      },
      "source": [
        "# The 2 quantities below are the step size per epoch for respectively the training set and validation set\n",
        "# We chose those quantities to make the steps depending on the number of samples on the set and on the batch size\n",
        "STEP_SIZE_TRAIN=train_gen.n//train_gen.batch_size\n",
        "STEP_SIZE_VALID=valid_gen.n//valid_gen.batch_size\n",
        "\n",
        "# Model training\n",
        "model.fit(x=train_dataset,\n",
        "          epochs=60,\n",
        "          steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=STEP_SIZE_VALID,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfpbH_gOr7nT"
      },
      "source": [
        "##Model prediction and results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLLHZVZyCEj7"
      },
      "source": [
        "# Model Evaluation\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "image_filenames = next(os.walk(test_dir))[2]\n",
        "\n",
        "predictions = {}\n",
        "for image_filename in image_filenames:\n",
        "  \n",
        "  # Image loading and converting to RGB mode\n",
        "  img = Image.open(os.path.join(test_dir,image_filename)).convert('RGB')\n",
        "\n",
        "  # Resizing it to make it suitable to the model structure\n",
        "  img = img.resize((img_h,img_w))\n",
        "  img_array = np.array(img)\n",
        "  img_array = img_array / 255\n",
        "  img_array = np.expand_dims(img_array, 0)\n",
        "\n",
        "  prediction = model.predict(img_array)\n",
        "\n",
        "  # Taking the argmax of the predictions made to have the class predicted\n",
        "  predictions[image_filename] = np.argmax(np.matrix(prediction))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1yeRLjCIoMw"
      },
      "source": [
        "# Function to create the CSV file with the results\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')\n",
        "\n",
        "# Create the CSV file from the predictions made and save it on our folder on Drive\n",
        "create_csv(predictions, '/content/drive/My Drive/no_mask_detector/results')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}